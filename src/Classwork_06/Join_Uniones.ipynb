{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e45f5401-1857-456f-8d7b-2ff9e0da5a3c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# üìò Sesi√≥n 6 ‚Äî Joins y Uniones de DataFrames en PySpark\n",
    "\n",
    "En la pr√°ctica real, los datos est√°n distribuidos en m√∫ltiples tablas. Para analizarlos en conjunto, necesitamos combinarlos. Esto se logra con los **joins**, tambi√©n conocidos como uniones de DataFrames.\n",
    "\n",
    "---\n",
    "\n",
    "## üîß Tipos de joins en PySpark\n",
    "\n",
    "| Tipo de Join     | Descripci√≥n                                              |\n",
    "|------------------|----------------------------------------------------------|\n",
    "| `inner`          | Devuelve solo las filas con claves coincidentes         |\n",
    "| `left`           | Devuelve todas las filas de la izquierda y las coincidentes de la derecha |\n",
    "| `right`          | Devuelve todas las filas de la derecha y las coincidentes de la izquierda |\n",
    "| `outer` (full)   | Devuelve todas las filas, coincidan o no                |\n",
    "| `semi` / `anti`  | Filtros especializados seg√∫n existencia de coincidencia |\n",
    "\n",
    "---\n",
    "\n",
    "## ‚ú® ¬øCu√°ndo se usan?\n",
    "\n",
    "- Para enriquecer una tabla con informaci√≥n adicional (cliente + transacci√≥n)\n",
    "- Para detectar registros sin coincidencias\n",
    "- Para construir un dataset unificado para an√°lisis o modelos\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "007eb296-f635-4856-a341-962d48bfa594",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Crear DataFrame de clientes\n",
    "df_clientes = spark.createDataFrame([\n",
    "    (1, \"Ana\"),\n",
    "    (2, \"Beto\"),\n",
    "    (3, \"Carla\"),\n",
    "    (4, \"David\")\n",
    "], [\"customerID\", \"nombre\"])\n",
    "\n",
    "# Crear DataFrame de transacciones\n",
    "df_transacciones = spark.createDataFrame([\n",
    "    (100, 1, 150),\n",
    "    (101, 2, 200),\n",
    "    (102, 5, 300),\n",
    "    (103, 6, 120)\n",
    "], [\"transactionID\", \"customerID\", \"monto\"])\n",
    "\n",
    "# INNER JOIN: solo clientes con transacciones\n",
    "print(\"üìå INNER JOIN\")\n",
    "df_inner = df_clientes.join(df_transacciones, on=\"customerID\", how=\"inner\")\n",
    "df_inner.show()\n",
    "# LEFT JOIN: todos los clientes, con o sin transacciones\n",
    "print(\"üìå LEFT JOIN\")\n",
    "df_left = df_clientes.alias(\"x\").join(\n",
    "            df_transacciones.alias(\"y\"), \n",
    "            on=\"customerID\", \n",
    "            how=\"left\"\n",
    "            )\n",
    "df_left.show()        \n",
    "\n",
    "# RIGHT JOIN: todas las transacciones, incluso si no hay cliente\n",
    "print(\"üìå RIGHT JOIN\")\n",
    "df_right = df_clientes.alias(\"x\").join(\n",
    "                df_transacciones.alias(\"y\"), \n",
    "                F.col(\"x.customerID\")==F.col(\"y.customerID\"), \n",
    "                how=\"right\").select(\n",
    "                    F.col(\"x.*\"),\n",
    "                    F.col(\"y.transactionID\"),\n",
    "                    F.col(\"y.monto\")\n",
    "                )\n",
    "df_right.show()\n",
    "\n",
    "# OUTER JOIN: todos los registros de ambos DataFrames\n",
    "print(\"üìå OUTER JOIN\")\n",
    "sql_query=\"\"\"\n",
    "select \n",
    "x.*, \n",
    "y.transactionID, y.monto\n",
    "from {clientes} x\n",
    "full outer join {transacciones} y ON x.customerID=y.customerID\n",
    "\"\"\"\n",
    "df_outer = spark.sql(sql_query, clientes=df_clientes, transacciones=df_transacciones)\n",
    "df_outer.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "df97c5c4-a3ef-4e11-bab3-4a69a3e20936",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    " Script en PySpark ‚Äî Ejemplo de UNION y UNION ALL con datos inventados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5c5b72a3-573e-4c29-809c-0cb3c7c55439",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Crear el primer DataFrame (clientes registrados en enero)\n",
    "df_enero = spark.createDataFrame([\n",
    "    (1, \"Ana\", \"Mexico\"),\n",
    "    (2, \"Luis\", \"Argentina\"),\n",
    "    (3, \"Camila\", \"Chile\")\n",
    "], [\"customerID\", \"nombre\", \"pais\"])\n",
    "\n",
    "# Crear el segundo DataFrame (clientes registrados en febrero)\n",
    "df_febrero = spark.createDataFrame([\n",
    "    (3, \"Camila\", \"Chile\"),         # repetido\n",
    "    (4, \"Pedro\", \"Colombia\"),\n",
    "    (5, \"Luc√≠a\", \"Per√∫\")\n",
    "], [\"customerID\", \"nombre\", \"pais\"])\n",
    "\n",
    "# UNION: elimina duplicados autom√°ticamente\n",
    "df_union = df_enero.union(df_febrero).distinct()\n",
    "\n",
    "# UNION ALL: conserva todos los registros, incluyendo duplicados\n",
    "df_union_all = df_enero.union(df_febrero)\n",
    "\n",
    "# Mostrar resultados\n",
    "print(\"üìå UNION (sin duplicados):\")\n",
    "df_union.show()\n",
    "\n",
    "print(\"üìå UNION ALL (con duplicados):\")\n",
    "df_union_all.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "aa1bcc0f-60c6-4410-83ce-d028c0e2c8d9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "‚úçÔ∏è Ejercicio ‚Äî Inner Join entre franquicias y transacciones\n",
    "\n",
    "üìò Objetivo\n",
    "\n",
    "Combinar la informaci√≥n de las franquicias con las transacciones de venta, para analizar cu√°ntas operaciones se realizaron por pa√≠s. Vamos a usar un inner join entre las tablas:\n",
    "- samples.bakehouse.sales_franchises\n",
    "- samples.bakehouse.sales_transactions\n",
    "\n",
    "üóÇÔ∏è Esquema de columnas relevantes\n",
    "\n",
    "sales_franchises\n",
    "```sh\n",
    "\t‚Ä¢\tfranchiseID\n",
    "\t‚Ä¢\tcountry\n",
    "\t‚Ä¢\tcity\n",
    "\t‚Ä¢\tstate\n",
    "  ```\n",
    "\n",
    "sales_transactions\n",
    "\n",
    "```sh\n",
    "\t‚Ä¢\ttransactionID\n",
    "\t‚Ä¢\tfranchiseID\n",
    "\t‚Ä¢\tdateTime\n",
    "\t‚Ä¢\ttotalPrice\n",
    "\t‚Ä¢\tproduct\n",
    "``` \n",
    "\n",
    "üéØ Enunciado del ejercicio:\n",
    "\n",
    "- 1 Realiz√° un inner join entre sales_franchises y sales_transactions usando la columna franchiseID como clave.\n",
    "- 2 Calcul√° cu√°ntas transacciones se realizaron por pa√≠s (country).\n",
    "- 3 Mostr√° los resultados en un DataFrame con las columnas:\n",
    "  - country\n",
    "  - rowsn (cantidad de transacciones)\n",
    "- 4 Orden√° los resultados de forma descendente seg√∫n el n√∫mero de transacciones."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 7780394177260641,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Join_Uniones",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
