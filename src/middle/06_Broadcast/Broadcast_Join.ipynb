{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ba6fed3b-cf70-48be-83c5-32183a230121",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import broadcast\n",
    "import time\n",
    "\n",
    "# =====================\n",
    "# Crear DataFrames simulados\n",
    "# =====================\n",
    "\n",
    "# Tabla grande (~10000000 registros)\n",
    "df_grande = spark.range(0, 10000000).repartition(2).withColumnRenamed(\"id\", \"cliente_id\")\n",
    "\n",
    "# Tabla peque√±a (~100 registros)\n",
    "data_pequena = [(i, f\"Segmento-{i%10}\") for i in range(10)]\n",
    "df_pequena = spark.createDataFrame(data_pequena, [\"cliente_id\", \"segmento\"]).coalesce(1)\n",
    "\n",
    "# =====================\n",
    "# 1. Join normal (sin optimizaci√≥n)\n",
    "# =====================\n",
    "print(\"üîÅ Join normal con PySpark:\")\n",
    "t1 = time.time()\n",
    "df_join_normal = df_grande.join(df_pequena, \"cliente_id\")\n",
    "df_join_normal.count()\n",
    "t2 = time.time()\n",
    "print(f\"‚è± Tiempo sin broadcast (PySpark): {t2 - t1:.2f} segundos\")\n",
    "\n",
    "# =====================\n",
    "# 2. Join con broadcast (PySpark)\n",
    "# =====================\n",
    "print(\"üöÄ Join con broadcast (PySpark):\")\n",
    "t3 = time.time()\n",
    "df_join_broadcast = df_grande.join(broadcast(df_pequena), \"cliente_id\")\n",
    "df_join_broadcast.count()\n",
    "t4 = time.time()\n",
    "print(f\"‚è± Tiempo con broadcast (PySpark): {t4 - t3:.2f} segundos\")\n",
    "\n",
    "# =====================\n",
    "# 3. Join normal con SQL\n",
    "# =====================\n",
    "\n",
    "# Registrar en SQL\n",
    "df_grande.createOrReplaceTempView(\"clientes\")\n",
    "df_pequena.createOrReplaceTempView(\"segmentos\")\n",
    "\n",
    "print(\"üß† Join normal con SQL:\")\n",
    "t5 = time.time()\n",
    "df_sql_normal = spark.sql(\"\"\"\n",
    "    SELECT *\n",
    "    FROM clientes c\n",
    "    JOIN segmentos s\n",
    "    ON c.cliente_id = s.cliente_id\n",
    "\"\"\")\n",
    "df_sql_normal.count()\n",
    "t6 = time.time()\n",
    "print(f\"‚è± Tiempo sin broadcast (SQL): {t6 - t5:.2f} segundos\")\n",
    "\n",
    "# =====================\n",
    "# 4. Join con broadcast en SQL (usando hint)\n",
    "# =====================\n",
    "print(\"‚ö° Join con broadcast en SQL:\")\n",
    "t7 = time.time()\n",
    "df_sql_broadcast = spark.sql(\"\"\"\n",
    "    SELECT /*+ BROADCAST(s) */\n",
    "           *\n",
    "    FROM clientes c\n",
    "    JOIN segmentos s\n",
    "    ON c.cliente_id = s.cliente_id\n",
    "\"\"\")\n",
    "df_sql_broadcast.count()\n",
    "t8 = time.time()\n",
    "print(f\"‚è± Tiempo con broadcast (SQL): {t8 - t7:.2f} segundos\")\n",
    "\n",
    "# =====================\n",
    "# Comparaci√≥n final\n",
    "# =====================\n",
    "print(\"\\nüìä Comparativa de tiempos:\")\n",
    "print(f\"- Join normal (PySpark):     {t2 - t1:.2f} s\")\n",
    "print(f\"- Join con broadcast (PySpark): {t4 - t3:.2f} s\")\n",
    "print(f\"- Join normal (SQL):         {t6 - t5:.2f} s\")\n",
    "print(f\"- Join con broadcast (SQL):  {t8 - t7:.2f} s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "57e1f3b7-a804-4dc8-97b9-32d259a9bf2a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# üöÄ Ejercicio ‚Äì Optimizaci√≥n de Joins con Broadcast y Particionamiento\n",
    "\n",
    "## üéØ Objetivo\n",
    "Comparar el rendimiento de un `join` tradicional versus un `broadcast join` en PySpark, utilizando un dataframe grande distribuido entre workers y un dataframe peque√±o replicado. Reflexionar sobre los beneficios de usar broadcast en casos adecuados.\n",
    "\n",
    "---\n",
    "\n",
    "## üìÅ Dataset\n",
    "\n",
    "Vas a trabajar con dos datasets simulados:\n",
    "\n",
    "- `df_grande`: un DataFrame grande (100.000 registros) con una sola columna `cliente_id`.\n",
    "- `df_pequeno`: un DataFrame peque√±o (100 registros) con columnas `cliente_id` y `segmento`.\n",
    "\n",
    "Ambos comparten la clave `cliente_id`.\n",
    "\n",
    "---\n",
    "\n",
    "## üõ† Actividades\n",
    "\n",
    "### 1Ô∏è‚É£ Crear ambos DataFrames\n",
    "\n",
    "```python\n",
    "from pyspark.sql.functions import broadcast\n",
    "\n",
    "# Dataset grande (distribuido en 2 particiones)\n",
    "df_grande = spark.range(0, 100000).repartition(2).withColumnRenamed(\"id\", \"cliente_id\")\n",
    "\n",
    "# Dataset peque√±o (simula cat√°logo de segmentos)\n",
    "data_pequeno = [(i, f\"Segmento-{i%10}\") for i in range(100)]\n",
    "df_pequeno = spark.createDataFrame(data_pequeno, [\"cliente_id\", \"segmento\"])\n",
    "```\n",
    "\n",
    "### 2Ô∏è‚É£ Realizar un join normal (sin broadcast)\n",
    "\n",
    "### 3Ô∏è‚É£ Realizar un broadcast join\n",
    "\n",
    "### 4Ô∏è‚É£ Visualizar el plan de ejecuci√≥n (opcional)\n",
    "\n",
    "```python\n",
    "df_join_broadcast.explain(extended=True)\n",
    "```\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Broadcast_Join",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
