{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "377a42bd-da09-415e-aa5d-0dedb02620a1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Tw7C6PZNFbUx",
    "outputId": "70704af8-bc6e-42bd-9a17-ea57713e48e7"
   },
   "outputs": [],
   "source": [
    "# ‚úÖ Importar librer√≠as necesarias\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf, col\n",
    "from pyspark.sql.types import StringType, StructType, StructField, BooleanType, IntegerType\n",
    "import re\n",
    "\n",
    "# ‚úÖ Crear DataFrame de ejemplo\n",
    "data = [\n",
    "    (1, \"  maria    jose  perez  \"),\n",
    "    (2, \"CARLOS ramirez\"),\n",
    "    (3, \"ana\"),\n",
    "    (4, \"jUAN  ignacio LOPEZ\"),\n",
    "    (5, \"luisa   fernanda   gomez  \"),\n",
    "]\n",
    "df = spark.createDataFrame(data, [\"id\", \"nombre_completo\"])\n",
    "\n",
    "df.show(truncate=False)\n",
    "\n",
    "# ‚úÖ UDF Estandariza los nombres y valida si tiene segundo nombre\n",
    "def procesar_nombre(nombre):\n",
    "    if not nombre:\n",
    "        return None\n",
    "    nombre_limpio = \" \".join(nombre.strip().lower().split())  # Quitar espacios extras y poner en min√∫scula\n",
    "    nombre_capitalizado = \" \".join([w.capitalize() for w in nombre_limpio.split()])\n",
    "    contiene_segundo_nombre = len(nombre_capitalizado.split()) > 2\n",
    "    return f\"{nombre_capitalizado} | Segundo nombre: {'S√≠' if contiene_segundo_nombre else 'No'}\"\n",
    "\n",
    "# ‚úÖ Registrar como UDF\n",
    "procesar_udf = udf(procesar_nombre, StringType())\n",
    "\n",
    "# ‚úÖ Aplicar UDF\n",
    "df_udf = df.withColumn(\"resultado_udf\", procesar_udf(col(\"nombre_completo\")))\n",
    "df_udf.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f91d6543-760a-4b3f-b897-451f61ad3378",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "p4YDpu5QF7WZ",
    "outputId": "65cf29a2-2759-48c1-ce4d-43658e19b599"
   },
   "outputs": [],
   "source": [
    "# ‚úÖ Registrar la UDF como funci√≥n SQL\n",
    "spark.udf.register(\"procesar_nombre_sql\", procesar_nombre, StringType())\n",
    "\n",
    "# ‚úÖ Crear una vista temporal para usar en SQL\n",
    "df.createOrReplaceTempView(\"clientes\")\n",
    "\n",
    "# ‚úÖ Usar la UDF en una consulta SQL\n",
    "df_sql = spark.sql(\"\"\"\n",
    "    SELECT\n",
    "        id,\n",
    "        nombre_completo,\n",
    "        procesar_nombre_sql(nombre_completo) AS resultado_sql\n",
    "    FROM clientes\n",
    "\"\"\")\n",
    "\n",
    "df_sql.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "934e4d56-12a7-478c-808b-f6921d5b373e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "zDA2r0KZH272"
   },
   "source": [
    "# üìå Ejercicio ‚Äì Limpieza de texto con funciones UDF\n",
    "\n",
    "## üîç Objetivo:\n",
    "Aplicar funciones UDF en PySpark para limpiar y normalizar textos provenientes de un archivo CSV con datos de contacto.\n",
    "\n",
    "‚∏ª\n",
    "\n",
    "## üß™ ¬øQu√© contiene el archivo?\n",
    "\n",
    "El dataframe tiene 2 columnas:\n",
    "- nombre_completo: incluye errores de may√∫sculas, espacios extras y valores vac√≠os.\n",
    "- email: contiene correos con formatos mixtos, nulos o mal escritos.\n",
    "\n",
    "```python\n",
    "data = [\n",
    "    ['carlos   RAMIREZ', 'pedro@mail'],\n",
    "    ['    ', None],\n",
    "    ['carlos   RAMIREZ', 'JUAN@correo.COM'],\n",
    "    ['PEDRO', 'ana_gomez@mail.com '],\n",
    "    ['ANA gomez', 'JUAN@correo.COM'],\n",
    "    ['PEDRO', ''],\n",
    "    ['ANA gomez', ' '],\n",
    "    ['  jUAN   p√©rez  ', ' carlos.ramirez@empresa.net'],\n",
    "    ['carlos   RAMIREZ', 'ana_gomez@mail.com '],\n",
    "    ['PEDRO', 'JUAN@correo.COM'],\n",
    "    ['carlos   RAMIREZ', 'JUAN@correo.COM'],\n",
    "    ['PEDRO', 'ana_gomez@mail.com '],\n",
    "    ['carlos   RAMIREZ', 'JUAN@correo.COM'],\n",
    "    ['    luisa fernanda   ', 'JUAN@correo.COM'],\n",
    "    ['ANA gomez', 'ana_gomez@mail.com '],\n",
    "    ['ANA gomez', 'JUAN@correo.COM'],\n",
    "    ['carlos   RAMIREZ', 'JUAN@correo.COM'],\n",
    "    ['  jUAN   p√©rez  ', 'marta.lopez@dominio.org'],\n",
    "    ['MARTA   lopez rodriguez', ''],\n",
    "    ['PEDRO', 'JUAN@correo.COM']\n",
    "]\n",
    "columns = ['nombre_completo', 'email']\n",
    "df_contactos = spark.createDataFrame(data, schema=columns)\n",
    "```\n",
    "\n",
    "## üõ† ¬øQu√© deben hacer?\n",
    "- Leer el archivo contactos_con_errores.csv usando PySpark.\n",
    "- Crear y registrar una funci√≥n UDF llamada limpiar_nombre que:\n",
    "  - Elimine espacios duplicados\n",
    "\t- Capitalice correctamente el nombre completo\n",
    "\t- Devuelva None si el texto est√° vac√≠o o es nulo\n",
    "- Crear otra UDF llamada normalizar_email que:\n",
    "\t- Elimine espacios en blanco\n",
    "\t- Pase a min√∫sculas\n",
    "\t- Detecte si tiene un formato v√°lido (contiene @ y .)\n",
    "\t- Devuelva \"email_invalido\" si no cumple el formato\n",
    "- Aplicar ambas UDFs al DataFrame original.\n",
    "- Mostrar el resultado final limpio usando .show().\n",
    "\n",
    "## funciones de referencia\n",
    "\n",
    "```python\n",
    "\n",
    "def limpiar_nombre(nombre):\n",
    "    if not nombre or nombre.strip() == \"\":\n",
    "        return None\n",
    "    nombre_limpio = \" \".join(nombre.strip().lower().split())\n",
    "    return \" \".join([w.capitalize() for w in nombre_limpio.split()])\n",
    "\n",
    "def normalizar_email(correo):\n",
    "    if not correo or correo.strip() == \"\":\n",
    "        return \"email_invalido\"\n",
    "    correo = correo.strip().lower()\n",
    "    return correo if re.match(r\"[^@]+@[^@]+\\.[^@]+\", correo) else \"email_invalido\"\n",
    "\n",
    "```\n",
    "\n",
    "üìÇ Entrega esperada\n",
    "- Script .py o notebook .ipynb\n",
    "- Aplicaci√≥n de ambas UDFs\n",
    "- Ejemplo con .show() de los resultados limpios\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a8ce93bc-300d-4bd3-a0d0-b5909f0670c0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "euOmlrGRIZyR"
   },
   "outputs": [],
   "source": [
    "# escriba aqui si script"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "UDF_Advanced",
   "widgets": {}
  },
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
