{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d4a2abd5-9cc8-482c-b196-84ae801f87d0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import rand, col, count\n",
    "from pyspark import StorageLevel\n",
    "import time\n",
    "\n",
    "\n",
    "# Crear un DataFrame de ejemplo\n",
    "df = spark.range(0, 10_000_000).withColumn(\"valor\", (rand() * 1000).cast(\"int\"))\n",
    "df_filtrado = df.filter(col(\"valor\") > 500)\n",
    "\n",
    "# Sin cache/persist - Primera acci√≥n\n",
    "start = time.time()\n",
    "df_filtrado.select(\"valor\").groupBy(\"valor\").agg(count(\"*\")).count()\n",
    "end = time.time()\n",
    "print(f\"Tiempo SIN cache/persist: {end - start:.2f} segundos\")\n",
    "\n",
    "# Con cache\n",
    "df_cache = df.filter(col(\"valor\") > 500).cache()\n",
    "\n",
    "# Acci√≥n para activar la cache\n",
    "df_cache.count()\n",
    "\n",
    "# Repetir una acci√≥n sobre el mismo DataFrame\n",
    "start = time.time()\n",
    "df_cache.select(\"valor\").groupBy(\"valor\").agg(count(\"*\")).count()\n",
    "end = time.time()\n",
    "print(f\"Tiempo CON cache: {end - start:.2f} segundos\")\n",
    "\n",
    "# Liberar memoria\n",
    "df_cache.unpersist()\n",
    "\n",
    "# Con persist (solo memoria)\n",
    "df_persist = df.filter(col(\"valor\") > 500).persist(StorageLevel.MEMORY_ONLY)\n",
    "\n",
    "# Acci√≥n para activar la persistencia\n",
    "df_persist.count()\n",
    "\n",
    "# Repetir otra acci√≥n\n",
    "start = time.time()\n",
    "df_persist.select(\"valor\").groupBy(\"valor\").agg(count(\"*\")).count()\n",
    "end = time.time()\n",
    "print(f\"Tiempo CON persistencia (MEMORY_ONLY): {end - start:.2f} segundos\")\n",
    "\n",
    "# Finalizar\n",
    "df_persist.unpersist()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a89ec5b6-35a7-4c49-aa3c-de044140aab9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# üß™ Ejercicio Pr√°ctico ‚Äì Join y Persistencia en PySpark\n",
    "\n",
    "## üéØ Objetivo\n",
    "Aplicar conceptos de joins entre m√∫ltiples tablas y explorar el impacto en el rendimiento al utilizar `cache()` y `persist()`.\n",
    "\n",
    "---\n",
    "\n",
    "## üìù Instrucciones\n",
    "\n",
    "### 1Ô∏è‚É£ Cargar las tablas del cat√°logo TPCH\n",
    "Utiliz√° las siguientes tablas disponibles en Databricks:\n",
    "- `samples.tpch.customer`\n",
    "- `samples.tpch.nation`\n",
    "- `samples.tpch.region`\n",
    "\n",
    "### 2Ô∏è‚É£ Realizar joins\n",
    "Un√≠ las tres tablas siguiendo estas condiciones:\n",
    "- `customer` ‚Üí `nation` usando `c_nationkey == n_nationkey`\n",
    "- `nation` ‚Üí `region` usando `n_regionkey == r_regionkey`\n",
    "\n",
    "Aplic√° un **filtro** para quedarte solo con los clientes cuya regi√≥n sea **'AMERICA'**.\n",
    "\n",
    "```python\n",
    "df_customer = spark.table(\"samples.tpch.customer\")\n",
    "df_nation = spark.table(\"samples.tpch.nation\")\n",
    "df_region = spark.table(\"samples.tpch.region\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 3Ô∏è‚É£ Medir tiempo sin persistencia\n",
    "\n",
    "---\n",
    "\n",
    "### 4Ô∏è‚É£ Medir tiempo usando `cache()`\n",
    "\n",
    "---\n",
    "\n",
    "### 5Ô∏è‚É£ Medir tiempo usando `persist()`\n",
    "\n",
    "---\n",
    "\n",
    "## üìÇ Entrega\n",
    "\n",
    "Sub√≠ un notebook `.ipynb` o script `.py` con:\n",
    "- El c√≥digo completo\n",
    "- Los tiempos registrados\n",
    "- Una breve reflexi√≥n comparando `cache()` y `persist()`\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ Criterios de evaluaci√≥n\n",
    "\n",
    "- ‚úî Correcta aplicaci√≥n de joins y filtros\n",
    "- ‚úî Uso adecuado de `cache()` y `persist()`\n",
    "- ‚úî Medici√≥n de tiempos clara\n",
    "- ‚úî An√°lisis final con observaciones propias\n",
    "\n",
    "---\n",
    "\n",
    "üí° *Tip:* Asegurate de que el cl√∫ster est√© activo y no saturado para obtener mediciones consistentes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "57304456-89df-42e0-a67c-d64da1345108",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Cache_Persist",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
