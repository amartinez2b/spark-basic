{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c27377ee-63a4-40c5-99f7-7ea4ac57a223",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, rand\n",
    "import time\n",
    "\n",
    "# Crear un DataFrame grande de prueba con 20 millones de filas\n",
    "df = spark.range(0, 20_000_000).withColumn(\"valor\", (rand() * 1000).cast(\"int\"))\n",
    "# Ver cu√°ntas particiones tiene inicialmente\n",
    "#print(f\"Particiones iniciales: {df.rdd.getNumPartitions()}\")\n",
    "# Reparticionar el DataFrame a un n√∫mero mayor de particiones\n",
    "df_repartitioned = df.repartition(20)\n",
    "#print(f\"Despu√©s de repartition(20): {df_repartitioned.rdd.getNumPartitions()}\")\n",
    "# Escribir en disco y medir el tiempo\n",
    "start = time.time()\n",
    "df_repartitioned.write.mode(\"overwrite\").parquet(\"dbfs:/content/sample_data/dataset_repartitioned\")\n",
    "end = time.time()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a28c66e4-190c-4ca1-98b3-c5845d091f1c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(f\"Tiempo de escritura con repartition: {end - start:.2f} segundos\")\n",
    "# Leer el dataset con muchas particiones\n",
    "df_read = spark.read.parquet(\"/content/sample_data/dataset_repartitioned\")\n",
    "print(f\"Particiones al leer: {df_read.rdd.getNumPartitions()}\")\n",
    "\n",
    "# Reducir el n√∫mero de particiones con coalesce\n",
    "df_coalesced = df_read.coalesce(1)\n",
    "print(f\"Despu√©s de coalesce(1): {df_coalesced.rdd.getNumPartitions()}\")\n",
    "# Escribir en disco y medir el tiempo\n",
    "start = time.time()\n",
    "df_coalesced.write.mode(\"overwrite\").parquet(\"/tmp/dataset_coalesced\")\n",
    "end = time.time()\n",
    "print(f\"Tiempo de escritura con coalesce: {end - start:.2f} segundos\")\n",
    "\n",
    "# Mantener la sesi√≥n abierta para observaci√≥n en UI si es necesario\n",
    "time.sleep(120)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a94c40e2-ce11-4993-9859-f009662ed9d8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# üß™ Ejercicio Pr√°ctico - Sesi√≥n 2: Particiones, Paralelismo y Tiempos de Ejecuci√≥n\n",
    "\n",
    "## üéØ Objetivo\n",
    "Practicar el control de particiones y medir el impacto en el rendimiento a trav√©s del tiempo de escritura con diferentes configuraciones.\n",
    "\n",
    "---\n",
    "\n",
    "## üìù Instrucciones\n",
    "\n",
    "### 1Ô∏è‚É£ Crear un DataFrame\n",
    "- Gener√° un `DataFrame` en PySpark con **5.000 registros** y al menos **3 columnas** (por ejemplo: n√∫mero, valor aleatorio, texto).\n",
    "\n",
    "---\n",
    "\n",
    "### 2Ô∏è‚É£ Escribir en formato Parquet con 3 particiones\n",
    "- Reparticionar el DataFrame en **3 particiones**.\n",
    "- Medir el tiempo de escritura en formato Parquet.\n",
    "\n",
    "---\n",
    "\n",
    "### 3Ô∏è‚É£ Leer y reducir a 1 partici√≥n\n",
    "- Le√© los archivos `.parquet` generados.\n",
    "- Us√° `coalesce(1)` para reducir a **una sola partici√≥n**.\n",
    "\n",
    "---\n",
    "\n",
    "### 4Ô∏è‚É£ Escribir como CSV con compresi√≥n BZIP2\n",
    "- Escrib√≠ el DataFrame como **un solo archivo CSV** comprimido en formato **bzip2**.\n",
    "- Med√≠ el tiempo de escritura.\n",
    "\n",
    "```python\n",
    "df_single.write.mode(\"overwrite\") \\\n",
    "    .option(\"compression\", \"bzip2\") \\\n",
    "    .csv(\"/tmp/df_csv_bzip\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üìÇ Entrega\n",
    "\n",
    "- Sub√≠ el script `.py` o notebook `.ipynb` con el c√≥digo completo.\n",
    "- Inclu√≠ los tiempos de ejecuci√≥n impresos y comentarios explicativos.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ Criterios de evaluaci√≥n\n",
    "\n",
    "- ‚úî Uso correcto de `repartition` y `coalesce`.\n",
    "- ‚úî Escritura en los formatos y compresi√≥n solicitados.\n",
    "- ‚úî Medici√≥n de tiempo correcta en cada escritura.\n",
    "- ‚úî C√≥digo limpio, comentado y funcional.\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Particiones_y_Paralelismo",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
